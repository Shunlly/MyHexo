---
title: 爬虫的简单介绍
date: 2021-10-17 15:31:01
tags: 爬虫
categories: 学习
keywords: 爬虫
description: 爬虫的记录
cover: https://typora-1303886849.cos.ap-guangzhou.myqcloud.com/typora%2F4327c995d22931a33f4ae872ee8605a42d8dc974.jpg%40942w_531h_progressive.jpg
top_img: https://typora-1303886849.cos.ap-guangzhou.myqcloud.com/typora%2F4327c995d22931a33f4ae872ee8605a42d8dc974.jpg%40942w_531h_progressive.jpg
---

# 爬虫的简单介绍

## 场景

对于爬取数据来说，有在不同的场景下爬取数据。绝大多数的情况下，要么是网页，要么是app。这里就简单的介绍一下这两个类别吧。

分享一个爬虫案例平台，可以上去测试各种各样的爬虫： <https://scrape.center>

java爬虫的一个样例，可以瞅瞅： <https://juejin.cn/post/6844904064736559117>

-   网页爬取

    -   服务端渲染

    -   客户端渲染

-   App爬取

    -   普通接口

    -   加密参数接口

    -   加密内容接口

    -   非常规协议接口

## 爬取——网页爬取

服务端渲染的意思就是页面的结果是由服务器渲染后返回的，有效信息包含在请求的HTML页面里面，比如猫眼电影这个站点。

客户端渲染的意思就是页面的主要内容是由JavaScript渲染而成，真实的数据是通过Ajax接口的形式获取的，比如淘宝、微博等等站点。

下图是一个电影网站的榜单数据，这是直接在html页面中渲染出来的：

![image-20221010104624247](https://typora-1303886849.cos.ap-guangzhou.myqcloud.com/typora/image-20221010104624247.png)

附上源代码：

```python
import requests  
from bs4 import BeautifulSoup  
import lxml  

  

def spdier_maoyan(url):  
    headers = {  
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",  
        "Accept-Encoding": "gzip, deflate, br",  
        "Accept-Language": "zh-CN,zh;q=0.9",  
        "Cookie": "UM_distinctid=180690fa304d50-049f7b7b2b113d-53580614-13c680-180690fa3057ff",  
        "Host": "ssr1.scrape.center",  
        "Referer": "https://ssr1.scrape.center/",  
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36",  
    }  
    response = requests.get(url, headers=headers)  
    text = response.text  
    soup = BeautifulSoup(response.text, 'lxml')  
    all_div = soup.find_all('div',attrs={"el-card item m-t is-hover-shadow"})  
    for div in all_div:  
        a = div.find('a')  
        title = div.find('h2')  
        print(url+a['href'])  
        print('title',title.text)  

  

# 按间距中的绿色按钮以运行脚本。  
if __name__ == '__main__':  
    url = 'https://ssr1.scrape.center'  
    spdier_maoyan(url)


```

服务端渲染的情况就比较简单了，用一些基本的 HTTP 请求库就可以实现爬取，如 urllib、urllib3、pycurl、hyper、requests、grab 等框架，其中应用最多的可能就是 requests 了。

客户端渲染的情况如下：

-   寻找Ajax接口，可以通过浏览器的开发者工具直接查看Ajax请求方式。

-   模拟浏览器进行爬取，这种可用于网页接口较为复杂的情况。例如。可以使用Selenium、异步Asyncio + Pyppeteer等等

-   复杂一点的，就是要获取到网页的js，然后执行js获取到数据

## 爬取——App爬取

这里分了几种情况：

-   对于普通无加密接口，这种直接通过抓包软件，拿到接口的具体请求形式就ok了。可用的抓包工具有Charles、Fiddler、mitmproxy。

-   对于一些加密参数的接口，一种方法是可以实时处理的，例如Fiddler、mitmproxy、Xposed等等，另一种方法是将加密逻辑破解，这里就涉及到反编译的技巧了

-   对于一些加密内容的忌口，接口返回结果完全看不懂是什么东西，可以使用可见即可爬的工具 Appium，也可以使用 Xposed 来 hook 获取渲染结果，也可以通过反编译和改写手机底层来实现破解。这里可以举一个例子，【得道App】，这个App的数据内容是加密的，可以拿这个软件试手，网上也有很多爬取【得道App】的例子。

## 反爬的一些常见情况

-   非浏览器检测：识别请求头Headers里面有咩有包含User-Agent

-   封Ip：一个Ip如果频繁访问，会被封禁

-   弹验证码框：输入验证码之后才能看到网页

## 反反爬

-   非浏览器检测

    -   这个相对来说是最简单的，可以自己做一个User-Agent的数组，用到时从中取一个就行，或者直接借用python的第三方模块【fake-useragent】。

-   封Ip

    -   这种情况的话，可以使用代理去访问，可以先去爬取代理网站，做一个代理池，每次切换不同的ip代理去访问网页，这样可以避免频繁访问封锁ip

-   弹验证码框，验证码分为非常多种，如普通图形验证码、算术题验证码、滑动验证码、点触验证码、手机验证码、扫二维码等。

    -   对于普通图形验证码，如果非常规整且没有变形或干扰，可以使用 OCR 识别，也可以使用机器学习、深度学习来进行模型训练，当然打码平台是最方便的方式。

    -   对于算术题验证码，推荐直接使用打码平台。

    -   对于滑动验证码，可以使用破解算法，也可以模拟滑动。后者的关键在于缺口的找寻，可以使用图片比对，也可以写基本的图形识别算法，也可以对接打码平台，也可以使用深度学习训练识别接口。

    -   对于点触验证码，推荐使用打码平台。

    -   对于手机验证码，可以使用验证码分发平台，也可以购买专门的收码设备，也可以人工验证。

    -   对于扫二维码，可以人工扫码，也可以对接打码平台。【这里对应的一个场景，就是微信公众号】

## 加速爬取

当爬取的数据量非常大时，如何高效快速地进行数据抓取是关键。常见的措施有多线程、多进程、异步、分布式、细节优化等。

这里就不多介绍了。

## 数据存储

这个根据具体实际情况，用不同的方式进行存储，有用Mysql、MongoDB这类数据进行数据存储的，也可以用TXT、Excel等形式进行数据存储。选哪一种都可以。

## 爬虫框架

主流的爬虫框架有Scrapy(Python) 、Pyspider(Python)、Apache Nutch(高大上)、WebMagic(Java爬虫框架)、WebCollector(Java爬虫框架)等等。

## 1、在公司接触过的爬虫

-   在安科院时，爬取抖音、微博、微信的数据

### 1. 1    抖音、微博、微信数据爬取

#### 1. 1. 1    微博

像微博的话，它的源网址是 <https://passport.weibo.cn/signin/login>。点击进入网址后，是需要进行登录的。下面这张图是登录之后的页面

![image-20221010104652413](https://typora-1303886849.cos.ap-guangzhou.myqcloud.com/typora/image-20221010104652413.png)

然后打开开发者工具去看请求参数，

![image-20221010104713891](https://typora-1303886849.cos.ap-guangzhou.myqcloud.com/typora/image-20221010104713891.png)

代码已经封存好了，直接输入uid就可以进行爬取了，这里附上gitlab上的项目链接 <https://github.com/dataabc/weiboSpider>

由于业务场景需要，需要视频的缩略图，我这里借用了第三方的工具和模块，取第一帧作为视频缩略图。附上代码，

```python
import ffmpy  
# 将视频的第一帧取下来，作为视频的缩略图存下来  
def get_thumbnail_from_video(video_path):  
    print("开始将视频转换为缩略图")  
    thumbnail_path = video_path.replace(".mp4", "_min.jpg")  
    ff = ffmpy.FFmpeg(  
        inputs={video_path: None},  
        outputs={thumbnail_path: ['-ss', '00:00:00.000', '-vframes', '1']}  
    )  
    ff.run()  
    print("转换为缩略图结束")  
    return thumbnail_path
```



#### 1. 1. 2    抖音

首先，获取源网页，这里就不用App进行爬取了，这里应用的通过手机App上的个人主页获取到抖音的链接，之后就可以通过这个链接进行分析，进行爬取了。就举一个例子吧，以目前最火的刘畊宏为例，

![image-20221010104735019](https://typora-1303886849.cos.ap-guangzhou.myqcloud.com/typora/image-20221010104735019.png)

![image-20221010104749532](https://typora-1303886849.cos.ap-guangzhou.myqcloud.com/typora/image-20221010104749532.png)

![image-20221010104803277](https://typora-1303886849.cos.ap-guangzhou.myqcloud.com/typora/image-20221010104803277.png)

在获取到链接后，将链接替换掉，运行脚本，就可以将数据爬取下来了。

![image-20221010104816216](https://typora-1303886849.cos.ap-guangzhou.myqcloud.com/typora/image-20221010104816216.png)

#!/usr/bin/python  

```python
# -*- coding: utf-8 -*-  
from bs4 import BeautifulSoup  
import requests  
import urllib.request  
import urllib  
import json  
import re  
import csv  
import os  
import uuid  
import logging  

headers = {  
    'accept-encoding': 'deflate',  
    'accept-language': 'zh-CN,zh;q=0.9',  
    'pragma': 'no-cache',  
    'cache-control': 'no-cache',  
    'upgrade-insecure-requests': '1',  
    'user-agent': "Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1",  
}  

HEADERS = {'user-agent': "Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1"}  

mapCode2Name = {"0xe602":"num_","0xe605":"num_3","0xe606":"num_4","0xe603":"num_1","0xe604":"num_2","0xe618":"num_","0xe619":"num_4","0xe60a":"num_8","0xe60b":"num_9","0xe60e":"num_","0xe60f":"num_5","0xe60c":"num_4",   
    "0xe60d":"num_1","0xe612":"num_6","0xe613":"num_8","0xe610":"num_3","0xe611":"num_2","0xe616":"num_1","0xe617":"num_3","0xe614":"num_9","0xe615":"num_7","0xe609":"num_7","0xe607":"num_5","0xe608":"num_6","0xe61b":"num_5",  
        "0xe61c":"num_8","0xe61a":"num_2","0xe61f":"num_6","0xe61d":"num_9","0xe61e":"num_7"}  
mapCode2Font = {"num_9":8,"num_5":5,"num_6":6,"num_":1,"num_7":9,"num_8":7,"num_1":0,"num_2":3,"num_3":2,"num_4":4}  

def getUserInfo(shared_url, **headers):  
    real_url = getRealAddress(shared_url)  
    parsed = urllib.parse.urlparse(real_url)  
    hostname = parsed.hostname  
    sec_uid = urllib.parse.parse_qs(parsed.query)['sec_uid']  
    user_info_url = "https://%s/web/api/v2/user/info/" % hostname  
    user_info_params = { 'sec_uid': sec_uid }  
    res = requests.get(user_info_url, headers=headers,  
                            params=user_info_params).json()  
    user_info = res['user_info']  
    user_avatar = user_info['avatar_larger']['url_list'][2]  
    user_nickname = user_info['nickname']  
    user_sign = user_info['signature']  
    user_id = user_info['unique_id']  
    count_of_videos = user_info['aweme_count']  
    follower_count = user_info['follower_count']  
    following_count = user_info['following_count']  
    zan_count = user_info['total_favorited']  
    like_count = user_info['favoriting_count']  
    return {'user_avatar':user_avatar, 'user_nickname':user_nickname, 'user_sign':user_sign, 'user_id':user_id,  
           'count_of_videos':count_of_videos, 'follower_count':follower_count, 'following_count':following_count,  
           'zan_count':zan_count, 'like_count':like_count, 'like_count':like_count}  

def getUserVideos(url):  
    # number = re.findall(r'share/user/(d+)', url)  
    number = url.split('share/user/')[1].split('?')[0]  
    if not len(number):  
        return  
    dytk = get_dytk(url)  
    # hostname = urllib.parse.urlparse(url).hostname  
    # if hostname != 't.tiktok.com' and not dytk:  
    #     return  
    user_id = number[0]  
    return getUserMedia(user_id, dytk, url)  


def getRealAddress(url):  
    if url.find('v.douyin.com') &lt; 0:  
        return url  
    res = requests.get(url, headers=headers, allow_redirects=False)  
    return res.headers['Location'] if res.status_code == 302 else None  


def get_dytk(url):  
    res = requests.get(url, headers=headers)  
    if not res:  
        return None  
    dytk = re.findall("dytk: '(.*)'", res.content.decode('utf-8'))  
    if len(dytk):  
        return dytk[0]  
    return None  

def getUserMedia(user_id, dytk, url):  
    videos = []  
    parsed = urllib.parse.urlparse(url)  
    hostname = parsed.hostname  
    sec_uid = urllib.parse.parse_qs(parsed.query)['sec_uid']  

    #signature = generateSignature(str(user_id))  
    user_video_url = "https://%s/web/api/v2/aweme/post/" % hostname  
    user_video_params = {  
        'sec_uid': sec_uid,  
        'count': '21',  
        'max_cursor': '0',  
        'aid': '1128',  
        '_signature': '2Vx9mxAZh0o-K4Wdv7NFKNlcfY',  
        'dytk': dytk  
    }  
    if hostname == 't.tiktok.com':  
        user_video_params.pop('dytk')  
        user_video_params['aid'] = '1180'  

    max_cursor, video_count = None, 0  
    while True:  
        if max_cursor:  
            user_video_params['max_cursor'] = str(max_cursor)  
        res = requests.get(user_video_url, headers=headers,  
                            params=user_video_params)  
        contentJson = json.loads(res.content.decode('utf-8'))  
        aweme_list = contentJson.get('aweme_list', [])  
        for aweme in aweme_list:  
            video_count += 1  
            aweme['hostname'] = hostname  
            video =  {  
                'addr': aweme['video']['play_addr']['url_list'][0],  
                'desc': aweme['desc'],  
                'duration': aweme['video']['duration'],  
                'cover': aweme['video']['cover']['url_list'][0],  
                'statistics': aweme['statistics']  
            }  
            videos.append(video)  
        if contentJson.get('has_more'):  
            max_cursor = contentJson.get('max_cursor')  
        else:  
            break  
    

    if video_count == 0:  
        print("There's no video in number %s." % user_id)  

    return videos  

  

def getHtml(url,**headers):  
    try:  
        req = urllib.request.Request(url,headers=headers)  
        resp = urllib.request.urlopen(req)  
        return str(resp.read(), 'utf-8')  
    except urllib.error.HTTPError as e:  
        print(e.msg)  
        return ''  

  

def woff2tff(ls):  
    res = ''  
    for s in ls:  
       res = res + formatNum(s)  
    return res  

def splitByChinese(s):  
    p = re.compile("[u4e00-u9fa5]", re.U)  
    return p.split(s)  

def isChinese(s):  
    p = re.compile("[u4e00-u9fa5]", re.U)  
    result = p.match(s)  
    if result :  
        return True  
    return False  
      

def formatNum(s):  
    if isChinese(s):  
        return ''  
    if len(s)&lt;8 or s.find("hzsdxe6") &lt; 0 :  
        return s  
    s1 = '0'+s[4:-1]  
    res = mapCode2Font[mapCode2Name[s1]]  
    return str(res)  


def getUserAll(shared_url):  
    profile = getUserInfo(shared_url, **HEADERS)  
    if profile:  
        videos = getUserVideos(getRealAddress(shared_url))  
        profile['videos'] = videos  
    return profile  

def download_csv(file_path,video):  
    result_headers = [('标题内容', 'desc'), ('视频时长', 'duration'),  
                      ('点赞数', 'share_count'),  
                      ('原始视频url', 'url'),  
                      ('视频存储位置','id'),  
                      ('缩略图','cover'),  
                      ('缩略图id','cover_id')]  

    result_data = [[w[kv[1]] for kv in result_headers]  
                   for w in video]  
    with open(file_path, 'a', encoding='utf-8-sig',  
              newline='') as f:  
        writer = csv.writer(f)  
        writer.writerows([[kv[0] for kv in result_headers]])  
        writer.writerows(result_data)  
    for i in video:  
        # 下载视频  
        download_video(i['url'],i['id'])  
        # 下载视频的缩略图  
        download_min_jpg(i['cover'],i['cover_id'])  
    logging.info(u'%d条微博写入csv文件完毕，保存路径：%s', len(video), file_path)  

def download_video(url,id):  
    """下载文件(图片/视频)"""  
    path = "/Users/mac/Desktop/data/douyin3/"  
    r = requests.get(url)  # 获取到目标视频的所有信息  
    with open(path+id+".mp4", 'wb') as f:  # 以二进制写的方式将r的二进制内容写入path  
        f.write(r.content)  
        f.close()  
        print("视频文件保存成功")  

def download_min_jpg(url,id):  
    path = "/Users/mac/Desktop/data/douyin3/"  
    r = requests.get(url)  # 获取到目标视频的所有信息  
    with open(path + id + ".jpg", 'wb') as f:  # 以二进制写的方式将r的二进制内容写入path  
        f.write(r.content)  
        f.close()  
        print("缩略图保存成功")  

if __name__ == '__main__':  
    userInfo = getUserAll("https://v.douyin.com/FNyoaSR/")  
    print(len(userInfo['videos']))  
    print(userInfo)  
    list = []  
    for i in userInfo['videos']:  
        # 点赞数  
        share_count = i['statistics']['share_count']  
        # 原始视频地址  
        url = i['addr']  
        #视频时长  
        duration = i['duration']  
        # 标题内容  将ascii码转为中文  
        desc = i['desc']  
        uid = str(uuid.uuid4())  
        suid = ''.join(uid.split('-'))  
        # 缩略图 url  
        cover =  i['cover']  
        # 缩略图id  
        cover_id = suid+"_min"  
        list1 = {'desc': desc, 'duration': duration, 'share_count': share_count,  
                 'url': url, 'id': suid, 'cover': cover, 'cover_id': cover_id}  
        list.append(list1)  
    #print(list)  
    filepath = '/Users/mac/Desktop/data/douyin3/douyin.csv'  
    download_csv(filepath,list)
```



#### 1. 1. 3 微信

在之前的操作中，将获取到的cookie存储到txt文档中，之后有用到的时候取，目前登录的cookie值是在2-4个小时内失效的。

在通过Selenium，控制浏览器后，微信会出现一个二维码页面，需要扫描二维码才能登录，这一步只能人工去解决。

![image-20221010104841428](https://typora-1303886849.cos.ap-guangzhou.myqcloud.com/typora/image-20221010104841428.png)

进入微信公众号平台后，找到相应的位置，剩下的是找到安科院下的两个微信公众号的请求链接地址，分析里面的参数。

![image-20221010104852717](https://typora-1303886849.cos.ap-guangzhou.myqcloud.com/typora/image-20221010104852717.png)

继续点击下一页，看看每一页哪些参数发生了变化，有没有规律，经分析得到了begin是变化的，并且是按照5的倍数递增的，之后就是请求每一页的数据，然后获取到发送的每一条数据的信息。在这期间，被封过几次号，爬取的太频繁的话，微信会对访问公众号进行限制，大概一天解封。

![image-20221010104902165](https://typora-1303886849.cos.ap-guangzhou.myqcloud.com/typora/image-20221010104902165.png)

找到规律后，就是循环替换请求参数，然后去访问获取到公众号发布文章的链接地址和时间。
